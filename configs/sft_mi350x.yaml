# MI350X Optimized SFT Configuration
# Target: 8x AMD MI350X (288GB VRAM each)
# Optimizations: Larger batch sizes, disabled gradient checkpointing

# Model arguments
model_name_or_path: Qwen/Qwen2.5-Math-7B
model_revision: main
torch_dtype: bfloat16
attn_implementation: eager  # Use eager for ROCm compatibility

# Data training arguments
chat_template: "{%- if messages[0]['role'] == 'system' %}\n    {{- '<|im_start|>system\\n' + messages[0]['content'] + '<|im_end|>\\n' }}\n{%- else %}\n    {{- '<|im_start|>system\\nYou are a helpful assistant that provides well-reasoned solutions. Structure your response into two sections: <think> for your reasoning process </think> and then provide your final answer.<|im_end|>\\n' }}\n{%- endif %}\n{%- for message in messages %}\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\") %}\n        {{- '<|im_start|>' + message.role + '\\n' + message.content + '<|im_end|>' + '\\n' }}\n    {%- endif %}\n{%- endfor %}\n{%- if add_generation_prompt %}\n    {{- '<|im_start|>assistant\\n' }}\n{%- endif %}\n"
dataset_name: open-r1/Mixture-of-Thoughts
dataset_config: all
dataset_num_proc: 16
eos_token: <|im_end|>

# SFT trainer config
bf16: true
do_eval: false
eval_strategy: 'no'

# MI350X Optimized: High throughput settings
gradient_accumulation_steps: 4
per_device_train_batch_size: 4

# Enable gradient checkpointing for long sequences
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false

hub_model_id: OpenR1-Distill-7B-MI350X
hub_strategy: every_save
learning_rate: 4.0e-05
log_level: info
logging_steps: 1
logging_strategy: steps
lr_scheduler_type: cosine_with_min_lr
lr_scheduler_kwargs:
  min_lr_rate: 0.1
packing: true
max_grad_norm: 0.2
max_length: 4096
max_steps: -1
num_train_epochs: 1
output_dir: /workspace/outputs/sft-7b
overwrite_output_dir: true
per_device_eval_batch_size: 2
push_to_hub: false
report_to: []
save_strategy: epoch
save_total_limit: 2
seed: 42
use_liger_kernel: true
warmup_ratio: 0.03
